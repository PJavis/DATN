apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-spark-config
  namespace: {{ .Release.Namespace }}
data:
  spark-defaults.conf: |
    spark.master                    spark://{{ .Release.Name }}-spark-master:7077
    spark.hadoop.fs.s3a.endpoint    {{ .Values.spark.minio.endpoint }}
    spark.hadoop.fs.s3a.access.key  {{ .Values.spark.minio.accessKey }}
    spark.hadoop.fs.s3a.secret.key  {{ .Values.spark.minio.secretKey }}
    spark.hadoop.fs.s3a.path.style.access true
    spark.hadoop.fs.s3a.impl        org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.sql.catalogImplementation hive
    spark.sql.hive.metastore.uris   {{ .Values.spark.hive.metastoreUri }}
    spark.sql.hive.metastore.version 3.1.3
    spark.sql.hive.metastore.jars   /opt/spark/jars/*

  compact_parquet.py: |-
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col
    import logging
    import sys

    # Cấu hình logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    logger = logging.getLogger("WeatherParquetCompaction")

    # Khởi tạo SparkSession với tài nguyên tối ưu
    try:
        logger.info("Khởi tạo SparkSession")
        spark = (
            SparkSession.builder
            .appName("WeatherParquetCompaction")
            .config("spark.hadoop.fs.s3a.endpoint", "http://data-lake-minio-headless:9000")
            .config("spark.hadoop.fs.s3a.access.key", "minio")
            .config("spark.hadoop.fs.s3a.secret.key", "minio123")
            .config("spark.hadoop.fs.s3a.path.style.access", "true")
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
            .config("spark.hadoop.fs.s3a.region", "us-east-1")
            .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
            .config("spark.sql.catalogImplementation", "hive")
            .config("spark.sql.hive.metastore.uris", "thrift://data-lake-hive:9083")
            .config("spark.sql.hive.metastore.version", "3.1.3")
            .config("spark.hadoop.hive.metastore.sasl.enabled", "false")
            .config("spark.jars", "/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.761.jar")
            .config("spark.sql.shuffle.partitions", "4")  # Giảm số phân vùng shuffle
            .config("spark.default.parallelism", "4")     # Giảm song song
            .config("spark.driver.memory", "2g")          # Tăng RAM driver
            .config("spark.executor.memory", "4g")        # Tăng RAM executor
            .config("spark.executor.cores", "2")          # Tăng CPU
            .config("spark.parquet.block.size", "33554432")  # Kích thước khối 32MB
            .config("spark.sql.adaptive.enabled", "true")    # Bật chế độ thích ứng
            .enableHiveSupport()
            .getOrCreate()
        )
        logger.info("Khởi tạo SparkSession thành công")
    except Exception as e:
        logger.error(f"Lỗi khởi tạo SparkSession: {str(e)}")
        raise e

    # Hàm đếm số file trong phân vùng
    def get_partition_file_count(spark, partition_path):
        try:
            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
            path = spark._jvm.org.apache.hadoop.fs.Path(partition_path)
            file_count = len([status for status in fs.listStatus(path) if not status.isDirectory()]) if fs.exists(path) else 0
            logger.info(f"Phát hiện {file_count} file trong {partition_path}")
            return file_count
        except Exception as e:
            logger.error(f"Lỗi đếm file trong {partition_path}: {str(e)}")
            return 0

    # Hàm nén phân vùng
    def compact_partition(country_code, year, month):
        partition_path = f"s3a://hive-warehouse/weather.db/weather_data/country_code={country_code}/year={year}/month={month}"
        file_count = get_partition_file_count(spark, partition_path)
        
        if file_count > 10:  # Tăng ngưỡng lên 10
            logger.info(f"Tiến hành nén phân vùng: country_code={country_code}, year={year}, month={month} với {file_count} file")
            try:
                # Tạo bảng tạm
                temp_table = f"temp_weather_{country_code}_{year}_{month}"
                df = spark.table("weather.weather_data").filter(
                    (col("country_code") == country_code) & 
                    (col("year") == year) & 
                    (col("month") == month)
                ).select("station_id", "record_date", "temperature_max", "temperature_min", 
                        "precipitation", "snow", "elevation", "name", "latitude", "longitude")
                
                df.coalesce(1).write.mode("overwrite").saveAsTable(temp_table)
                spark.sql(f"INSERT OVERWRITE TABLE weather.weather_data PARTITION (country_code='{country_code}', year={year}, month={month}) SELECT * FROM {temp_table}")
                spark.sql(f"DROP TABLE {temp_table}")
                
                logger.info(f"Hoàn tất nén phân vùng: country_code={country_code}, year={year}, month={month}")
            except Exception as e:
                logger.error(f"Lỗi nén phân vùng {country_code}/{year}/{month}: {str(e)}")
                raise e
        else:
            logger.info(f"Không nén phân vùng: country_code={country_code}, year={year}, month={month} với {file_count} file")

    # Lấy danh sách phân vùng (giảm xuống 1 để thử nghiệm)
    try:
        logger.info("Lấy danh sách phân vùng từ weather.weather_data")
        partitions = spark.sql("""
            SELECT country_code, year, month
            FROM weather.weather_data
            GROUP BY country_code, year, month
            LIMIT 1  # Giảm xuống 1 phân vùng để thử nghiệm
        """).collect()
    except Exception as e:
        logger.error(f"Lỗi lấy danh sách phân vùng: {str(e)}")
        partitions = []

    # Nén từng phân vùng
    for row in partitions:
        compact_partition(row["country_code"], row["year"], row["month"])

    # Dừng SparkSession
    try:
        logger.info("Dừng SparkSession")
        spark.stop()
    except Exception as e:
        logger.error(f"Lỗi dừng SparkSession: {str(e)}")
        raise e

  spark_streaming.py: |-
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, year, month, dayofmonth, when, from_json
    from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType

    # =========================
    # 1. SparkSession
    # =========================
    spark = (
        SparkSession.builder.appName("WeatherStreamingToMinIO")
        .config("spark.hadoop.fs.s3a.endpoint", "http://data-lake-minio-headless:9000")
        .config("spark.hadoop.fs.s3a.access.key", "minio")
        .config("spark.hadoop.fs.s3a.secret.key", "minio123")
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
        .config("spark.hadoop.fs.s3a.region", "us-east-1")
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
        .config("spark.sql.catalogImplementation", "hive")
        .config("spark.sql.hive.metastore.uris", "thrift://data-lake-hive:9083")
        .config("spark.sql.hive.metastore.version", "3.1.3")
        .config("spark.sql.hive.metastore.jars", "file://opt/spark/jars/*")
        .config("spark.hadoop.hive.metastore.sasl.enabled", "false")
        .config("spark.jars", "/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.5.jar,/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.761.jar,/opt/spark/jars/postgresql-42.7.3.jar,/opt/spark/jars/kafka-clients-3.4.1.jar,/opt/spark/jars/spark-streaming-kafka-0-10_2.12-3.5.5.jar,/opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.5.jar,/opt/spark/jars/jsr305-3.0.0.jar,/opt/spark/jars/commons-pool2-2.11.1.jar,/opt/spark/jars/lz4-java-1.8.0.jar,/opt/spark/jars/snappy-java-1.1.10.5.jar")
        .enableHiveSupport()
        .getOrCreate()
    )

    # spark.sparkContext.setLogLevel("DEBUG")

    # Kiểm tra Kafka source
    print("Checking Kafka Source Provider:")
    try:
        spark.readStream.format("kafka") \
            .option("kafka.bootstrap.servers", "data-lake-kafka-headless:9092") \
            .option("subscribe", "weather-topic") \
            .load()
        print("Kafka source loaded successfully")
    except Exception as e:
        print(f"Error loading Kafka source: {str(e)}")

    # =========================
    # 2. Schema cho Kafka message
    # =========================
    schema = StructType(
        [
            StructField("ghcn_din", StringType(), False),
            StructField("date", StringType(), False),
            StructField("prcp", FloatType(), True),
            StructField("snow", FloatType(), True),
            StructField("tmax", FloatType(), True),
            StructField("tmin", FloatType(), True),
            StructField("elevation", FloatType(), True),
            StructField("name", StringType(), True),
            StructField("latitude", FloatType(), True),
            StructField("longitude", FloatType(), True),
            StructField("country_code", StringType(), False),
        ]
    )

    # =========================
    # 3. Đọc Kafka
    # =========================
    kafka_df = (
        spark.readStream.format("kafka")
        .option("kafka.bootstrap.servers", "data-lake-kafka-headless:9092")
        .option("subscribe", "weather-topic")
        .option("startingOffsets", "latest")
        .option("failOnDataLoss", "false")
        .option("maxOffsetsPerTrigger", 10000)
        .load()
    )

    # Parse dữ liệu
    parsed_df = (
        kafka_df.selectExpr("CAST(value AS STRING)")
        .select(from_json(col("value"), schema).alias("data"))
        .select("data.*")
    )

    # Xử lý dữ liệu
    processed_df = (
        parsed_df
        .withColumn("prcp", when(col("prcp").isNull(), 0.0).otherwise(col("prcp")))
        .withColumn("snow", when(col("snow").isNull(), 0.0).otherwise(col("snow")))
        .withColumn("tmax", when(col("tmax").isNull(), 0.0).otherwise(col("tmax")))
        .withColumn("tmin", when(col("tmin").isNull(), 0.0).otherwise(col("tmin")))
        .withColumn("elevation", when(col("elevation").isNull(), 0.0).otherwise(col("elevation")))
        .withColumn("name", when(col("name").isNull(), "").otherwise(col("name")))
        .withColumn("latitude", when(col("latitude").isNull(), 0.0).otherwise(col("latitude")))
        .withColumn("longitude", when(col("longitude").isNull(), 0.0).otherwise(col("longitude")))
        .withColumn("country_code", when(col("country_code").isNull(), "UNKNOWN").otherwise(col("country_code")))
        .withColumn("record_date", col("date").cast("date"))  # Cast sang DATE
        .withColumn("year", year(col("record_date")))
        .withColumn("month", month(col("record_date")))
        .select(
            col("ghcn_din").alias("station_id"),
            col("record_date"),
            col("tmax").alias("temperature_max"),
            col("prcp").alias("precipitation"),
            col("snow"),
            col("tmin").alias("temperature_min"),
            col("elevation"),
            col("name"),
            col("latitude"),
            col("longitude"),
            col("country_code"),
            col("year"),
            col("month")
        )
    )

    output_df = processed_df.repartition(4)

    # Xử lý batch
    def process_batch(batch_df, batch_id):
        print(f"Processing batch {batch_id}...")
        try:
            batch_df.write.partitionBy("country_code", "year", "month") \
                .format("parquet") \
                .option("compression", "snappy") \
                .mode("append") \
                .save("s3a://hive-warehouse/weather.db/weather_data")
            print(f"Batch {batch_id} saved to MinIO at s3a://hive-warehouse/weather.db/weather_data")

            partitions = batch_df.select("country_code", "year", "month").distinct().collect()
            for row in partitions:
                spark.sql(f"""
                    ALTER TABLE weather.weather_data
                    ADD IF NOT EXISTS PARTITION (
                        country_code='{row["country_code"]}',
                        year={row["year"]},
                        month={row["month"]}
                    )
                    LOCATION 's3a://hive-warehouse/weather.db/weather_data/country_code={row["country_code"]}/year={row["year"]}/month={row["month"]}'
                """)
            print(f"Batch {batch_id} partitions updated in Hive")
        except Exception as e:
            print(f"Error in batch {batch_id}: {str(e)}")

    # WriteStream
    query = (
        output_df.writeStream.foreachBatch(process_batch)
        .option("checkpointLocation", "s3a://hive-warehouse/spark/checkpoint")
        .outputMode("append")
        .trigger(processingTime="10 seconds")
        .start()
    )

    query.awaitTermination()